{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7ece90-1d84-413c-b003-e260b343574d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f19ad1-bd6d-4e7e-ad23-50c13a39ee96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.stats import chi2\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeRegressor, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier, AdaBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Suppress all warnings\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7862336d-5a4c-4270-bd82-ccafc11cfe0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated chunk size: 11805 rows per process\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# ✅ Define the GCS Parquet file path\n",
    "parquet_gcs_path = \"gs://demo-shubham/scaled-data.parquet\"\n",
    "\n",
    "# ✅ Read a small sample to check memory usage\n",
    "sample_df = pd.read_parquet(parquet_gcs_path, storage_options={\"token\": \"cloud\"}, engine=\"pyarrow\")\n",
    "mem_per_row = sample_df.memory_usage(deep=True).sum() / len(sample_df)\n",
    "\n",
    "# ✅ Estimate number of rows per chunk (keeping memory usage < 1GB per process)\n",
    "target_memory_per_core = 1 * 1024**2  # 1GB per process\n",
    "chunk_size = int(target_memory_per_core / mem_per_row)\n",
    "print(f\"Estimated chunk size: {chunk_size} rows per process\")\n",
    "\n",
    "# ✅ Read full dataset in chunks\n",
    "df = pd.read_parquet(parquet_gcs_path, storage_options={\"token\": \"cloud\"}, engine=\"pyarrow\")\n",
    "\n",
    "# ✅ Split into chunks for parallel processing\n",
    "num_partitions = min(cpu_count(), 8)  # Use all available vCPUs\n",
    "df_split = np.array_split(df, num_partitions)\n",
    "\n",
    "# ✅ Define processing function\n",
    "def process_chunk(df_chunk):\n",
    "    # Example: Compute summary statistics\n",
    "    return df_chunk.describe()\n",
    "\n",
    "# ✅ Use multiprocessing to process chunks in parallel\n",
    "with Pool(num_partitions) as pool:\n",
    "    results = pool.map(process_chunk, df_split)\n",
    "\n",
    "# ✅ Combine processed results\n",
    "final_result = pd.concat(results)\n",
    "#print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8e2fc6-6743-4ada-afa0-2468cc92f576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Sampled and Processed in Parallel\n"
     ]
    }
   ],
   "source": [
    "# ✅ Split into chunks for parallel processing\n",
    "num_partitions = min(cpu_count(), 8)  # Use all available vCPUs\n",
    "df_split = np.array_split(df, num_partitions)\n",
    "\n",
    "# ✅ Define function to process each chunk\n",
    "def process_chunk(df_chunk):\n",
    "    sampled_chunk = df_chunk.sample(frac=0.1, random_state=42)  # Sample 10%\n",
    "    X_chunk = sampled_chunk.drop(columns=[\"Healthy life\\nexpectancy\"])\n",
    "    y_chunk = sampled_chunk[\"Healthy life\\nexpectancy\"]\n",
    "    return X_chunk, y_chunk\n",
    "\n",
    "# ✅ Use multiprocessing to process chunks in parallel\n",
    "with Pool(num_partitions) as pool:\n",
    "    results = pool.map(process_chunk, df_split)\n",
    "\n",
    "# ✅ Combine sampled data from all chunks\n",
    "X_sampled = pd.concat([r[0] for r in results])\n",
    "y_sampled = pd.concat([r[1] for r in results])\n",
    "\n",
    "print(\"✅ Data Sampled and Processed in Parallel\")\n",
    "#print(X_sampled.head(), y_sampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f279c12e-2f6f-4ce1-a8dc-f2022934bad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# ✅ Train/Test Split (remains Dask arrays)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42,shuffle=True)\n",
    "\n",
    "# ✅ Compute in **small batches** to avoid memory overload\n",
    "batch_size = 50000  # Adjust based on available memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46df39c4-39dd-4947-983c-992279e5910b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Train the model using Joblib's parallel backend\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):  \u001b[38;5;66;03m# Efficient multiprocessing\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Model Training Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:360\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    927\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    928\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    929\u001b[0m )\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m--> 931\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[0;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)"
     ]
    }
   ],
   "source": [
    "# ✅ Determine task type (Classification vs Regression)\n",
    "if np.unique(y_train).size <= 10:\n",
    "    model = ExtraTreesClassifier(n_jobs=8, random_state=42, n_estimators=10)\n",
    "    task_type = \"Classification\"\n",
    "else:\n",
    "    model = ExtraTreesRegressor(n_jobs=8, random_state=42, n_estimators=10)\n",
    "    task_type = \"Regression\"\n",
    "\n",
    "# ✅ Train the model using Joblib's parallel backend\n",
    "with parallel_backend(\"loky\", n_jobs=4):  # Efficient multiprocessing\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"✅ {task_type} Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e710c0ec-859e-4b27-96d3-6438c3186c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def select_top_features(model, X, min_features=5, plot=True):\n",
    "    \"\"\"\n",
    "    Selects top features based on feature importance and generates a bar chart.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A trained model with `feature_importances_` or `coef_` attribute.\n",
    "    - X: A Pandas or Dask DataFrame of input features.\n",
    "    - min_features: Minimum number of features to always select.\n",
    "    - plot: If True, generates a bar chart of feature importance.\n",
    "\n",
    "    Returns:\n",
    "    - List of selected feature names.\n",
    "    \"\"\"\n",
    "    # Check if model has feature importances or coefficients\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        feature_importances = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):  # Handle linear models\n",
    "        feature_importances = abs(model.coef_).flatten()\n",
    "    else:\n",
    "        raise ValueError(\"Model does not support feature importances or coefficients.\")\n",
    "\n",
    "    # Convert Dask to Pandas if necessary\n",
    "    if isinstance(X, dd.DataFrame):\n",
    "        X = X.compute()\n",
    "\n",
    "    # Create a DataFrame with feature names and importance scores\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    total_features = len(importance_df)\n",
    "\n",
    "    # Calculate the number of features to select (minimum + top 60%)\n",
    "    additional_features = math.ceil(0.6 * total_features)\n",
    "    selected_features_count = min(total_features, max(min_features, additional_features + min_features))\n",
    "\n",
    "    # Select top features\n",
    "    selected_features = importance_df.head(selected_features_count)['Feature'].tolist()\n",
    "\n",
    "    # Generate bar chart if enabled\n",
    "    if plot:\n",
    "        fig = px.bar(importance_df.head(selected_features_count), \n",
    "                     x=\"Importance\", \n",
    "                     y=\"Feature\", \n",
    "                     orientation=\"h\",\n",
    "                     title=\"Feature Importance\",\n",
    "                     labels={\"Importance\": \"Importance Score\", \"Feature\": \"Features\"},\n",
    "                     color=\"Importance\",\n",
    "                     color_continuous_scale=\"blues\")\n",
    "        fig.update_layout(yaxis=dict(categoryorder=\"total ascending\"))  # Sort bars\n",
    "        fig.show()\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Example usage:\n",
    "selected_features = select_top_features(model, X_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef41cfe-699a-4986-8e6d-235bfafe62c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_train_model(X, y, batch_size=50000, min_features=5):\n",
    "    \"\"\"\n",
    "    Train models using batch processing.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "        \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "        \"ExtraTreesRegressor\": ExtraTreesRegressor(n_estimators=10, random_state=42),\n",
    "        \"AdaBoostRegressor\": AdaBoostRegressor(n_estimators=10, random_state=42),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    final_selected_features = None\n",
    "    num_rows = X.shape[0]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} in batches...\")\n",
    "\n",
    "        batch_scores = []\n",
    "        selected_features = None\n",
    "        first_batch = True\n",
    "\n",
    "        # Iterate over data in batches\n",
    "        for i in range(0, num_rows, batch_size):\n",
    "            X_batch = X.iloc[i:i + batch_size]\n",
    "            y_batch = y.iloc[i:i + batch_size]\n",
    "\n",
    "            # Train model on batch\n",
    "            model.fit(X_batch, y_batch)\n",
    "\n",
    "            if first_batch and hasattr(model, \"feature_importances_\"):\n",
    "                selected_features = select_top_features(model, X_batch, min_features)\n",
    "                first_batch = False\n",
    "\n",
    "            # Use selected features for training\n",
    "            if selected_features is not None:\n",
    "                X_batch_selected = X_batch[selected_features]\n",
    "            else:\n",
    "                X_batch_selected = X_batch\n",
    "\n",
    "            # Re-train model with selected features\n",
    "            model.fit(X_batch_selected, y_batch)\n",
    "            batch_score = model.score(X_batch_selected, y_batch)\n",
    "            batch_scores.append(batch_score)\n",
    "\n",
    "        avg_score = np.mean(batch_scores)\n",
    "        results[model_name] = avg_score\n",
    "\n",
    "        if results[model_name] == max(results.values()):\n",
    "            final_selected_features = selected_features\n",
    "\n",
    "    # Select the best model\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    best_model_score = results[best_model_name]\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "    print(f\"Best model: {best_model_name} with avg. score: {best_model_score:.4f}\")\n",
    "    print(f\"Selected Features: {final_selected_features}\")\n",
    "\n",
    "    return best_model, best_model_name, best_model_score, final_selected_features\n",
    "\n",
    "# Example Usage\n",
    "# Assuming df is a Pandas DataFrame\n",
    "y = df[\"Healthy life\\nexpectancy\"]\n",
    "X = df.drop(columns=[\"Healthy life\\nexpectancy\"])\n",
    "\n",
    "best_model, model_name, model_score, top_features = batch_train_model(X, y, batch_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03dea52b-8976-4645-908e-7a17bb27e01b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Prediction.ipynb to script\n",
      "[NbConvertApp] Writing 9864 bytes to Prediction.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Prediction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcb541-2d38-4af4-b130-f35439df39e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
