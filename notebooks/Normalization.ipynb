{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4441abb3-25b6-4f57-8c89-a4ecb815a78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:36989' processes=8 threads=8, memory=22.35 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 15:20:32,499 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 5b4abfb0475f1c522e21e7f5294fe082 initialized by task ('shuffle-transfer-5b4abfb0475f1c522e21e7f5294fe082', 0) executed on worker tcp://127.0.0.1:41161\n",
      "2025-02-25 15:20:34,145 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 5b4abfb0475f1c522e21e7f5294fe082 deactivated due to stimulus 'task-finished-1740496834.1424341'\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "import multiprocessing\n",
    "\n",
    "# Automatically get the number of available CPUs\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "# Create a Dask client using all available vCPUs\n",
    "client = Client(n_workers=8, threads_per_worker=1, memory_limit='3GB', timeout=\"120s\")\n",
    "\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fbbce1-ce28-4991-b8bd-283b1064c12d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from scipy.stats import normaltest\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42fd33d-587a-4b55-8367-8804a2f268fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Country (region)  Ladder  SD of Ladder  Positive affect  Negative affect  \\\n",
      "0          Finland       1             4             41.0             10.0   \n",
      "1          Denmark       2            13             24.0             26.0   \n",
      "\n",
      "   Social support  Freedom  Corruption  Generosity  Log of GDP\\nper capita  \\\n",
      "0             2.0      5.0         4.0        47.0                    22.0   \n",
      "1             4.0      6.0         3.0        22.0                    14.0   \n",
      "\n",
      "   Healthy life\\nexpectancy  \n",
      "0                      27.0  \n",
      "1                      23.0  \n",
      "['Country (region)', 'Ladder', 'SD of Ladder', 'Positive affect', 'Negative affect', 'Social support', 'Freedom', 'Corruption', 'Generosity', 'Log of GDP\\nper capita', 'Healthy life\\nexpectancy']\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# ✅ Define the GCS path for the Parquet file\n",
    "parquet_gcs_path = \"gs://demo-shubham/df.parquet\"\n",
    "\n",
    "\n",
    "df = dd.read_parquet(\n",
    "    parquet_gcs_path,\n",
    "    blocksize=\"50MB\",  \n",
    "    assume_missing=True,  # Ensures int columns with missing values convert to float\n",
    "    low_memory=False  # Reduces memory overhead\n",
    ")\n",
    "# ✅ Show a preview (Dask loads data lazily)\n",
    "print(df.head(2))  # Reads only a small portion\n",
    "\n",
    "# ✅ Check column names\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f385d7-13f5-4144-815b-a9ee9ab20f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ✅ Persist the DataFrame to keep it in distributed memory\n",
    "df = df.persist()  # ✅ Reduce partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a51f447-fd7a-44d0-a2bc-91d712ac8510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify columns with all NaN values\n",
    "columns_to_drop = [col for col in df.columns if df[col].isna().all().compute()]\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df9aa29-37d3-4a52-905a-2b9c214b7144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# ✅ Automatically detect and convert object columns to datetime (ISO 8601 format)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':  # Check for potential date-like columns\n",
    "        df[col] = dd.to_datetime(df[col], errors='coerce')  # Convert to datetime\n",
    "\n",
    "# ✅ Standardize datetime format (Ensuring proper execution across partitions)\n",
    "df = df.map_partitions(lambda d: d.assign(\n",
    "    **{col: d[col].dt.strftime('%Y-%m-%d %H:%M:%S') for col in d.select_dtypes(include=['datetime64']).columns}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4af874-e5bb-4ebe-8e79-dc8c4bc04707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: Index(['Ladder', 'SD of Ladder'], dtype='object')\n",
      "Imputed median for Ladder: 78.5\n",
      "Imputed median for SD of Ladder: 78.5\n",
      "Processing batch: Index(['Positive affect', 'Negative affect'], dtype='object')\n",
      "Imputed median for Positive affect: 78.0\n",
      "Imputed median for Negative affect: 78.0\n",
      "Processing batch: Index(['Social support', 'Freedom'], dtype='object')\n",
      "Imputed median for Social support: 78.0\n",
      "Imputed median for Freedom: 78.0\n",
      "Processing batch: Index(['Corruption', 'Generosity'], dtype='object')\n",
      "Imputed median for Corruption: 74.5\n",
      "Imputed median for Generosity: 78.0\n",
      "Processing batch: Index(['Log of GDP\\nper capita', 'Healthy life\\nexpectancy'], dtype='object')\n",
      "Imputed median for Log of GDP\n",
      "per capita: 76.5\n",
      "Imputed median for Healthy life\n",
      "expectancy: 75.5\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2  # Adjust based on available memory\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[\"float64\",\"int64\"]).columns\n",
    "\n",
    "# Convert to a Dask DataFrame if not already\n",
    "if not isinstance(df, dd.DataFrame):\n",
    "    df = dd.from_pandas(df, npartitions=10)  # Adjust partitions based on your dataset size\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, len(numerical_cols), batch_size):\n",
    "    batch = numerical_cols[i : i + batch_size]  # Get the batch of columns\n",
    "    print(f\"Processing batch: {batch}\")\n",
    "\n",
    "    # Compute median for the batch at once (more efficient than per column)\n",
    "    median_values = df[batch].median_approximate().compute()\n",
    "\n",
    "    # Fill missing values for each column in the batch\n",
    "    for col in batch:\n",
    "        df[col] = df[col].fillna(median_values[col])\n",
    "        print(f\"Imputed median for {col}: {median_values[col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8334fe2-0171-44c1-82d1-6c44b9854c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_outliers_dask(df, method=None, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detects outliers using IQR or Z-score, with an option to auto-select the best method.\n",
    "    \"\"\"\n",
    "    outlier_percentages = {}\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    if method is None:\n",
    "        # Take a small sample (10%) efficiently using random_split\n",
    "        sample_frac = 0.1\n",
    "        df_sample, _ = df.random_split([sample_frac, 1 - sample_frac])\n",
    "        df_sample = df_sample.compute()  # Convert sample to Pandas DataFrame\n",
    "\n",
    "        normality_pvals = df_sample[numeric_cols].apply(lambda x: normaltest(x.dropna())[1])\n",
    "\n",
    "        if (normality_pvals > 0.05).all():  \n",
    "            method = \"zscore\"  # If p > 0.05, assume normal distribution\n",
    "        else:\n",
    "            method = \"iqr\"  # Otherwise, assume non-normal distribution\n",
    "\n",
    "        print(f\"Auto-selected method: {method}\")\n",
    "\n",
    "    # Now, use IQR or Z-score as before\n",
    "    if method == \"iqr\":\n",
    "        for col in numeric_cols:\n",
    "            quantiles = df[col].quantile([0.25, 0.75]).compute()\n",
    "            Q1, Q3 = quantiles.loc[0.25], quantiles.loc[0.75]\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "            mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            outlier_count = mask.sum().compute()\n",
    "            total_rows = len(df)\n",
    "            outlier_percentages[col] = (outlier_count / total_rows) * 100\n",
    "\n",
    "    elif method == \"zscore\":\n",
    "        for col in numeric_cols:\n",
    "            mean, std = df[col].mean().compute(), df[col].std().compute()\n",
    "            mask = abs((df[col] - mean) / std) > threshold\n",
    "            outlier_count = mask.sum().compute()\n",
    "            total_rows = len(df)\n",
    "            outlier_percentages[col] = (outlier_count / total_rows) * 100\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'iqr' or 'zscore'.\")\n",
    "\n",
    "    return outlier_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f798f1-4d25-4cce-ada4-4a0d43b77128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_or_winsorize_dask(df, outlier_percentages, threshold=5):\n",
    "    \"\"\"\n",
    "    Cleans or applies Winsorization based on outlier percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Dask DataFrame\n",
    "    - outlier_percentages: Dictionary of outlier percentages per column.\n",
    "    - threshold: Percentage threshold to decide between cleaning or Winsorization.\n",
    "    \n",
    "    Returns:\n",
    "    - Dask DataFrame with outliers cleaned or Winsorized.\n",
    "    \"\"\"\n",
    "    numeric_cols = list(outlier_percentages.keys())\n",
    "\n",
    "    # Compute IQR bounds once for all columns\n",
    "    stats = df[numeric_cols].quantile([0.25, 0.75]).compute()\n",
    "    Q1, Q3 = stats.loc[0.25], stats.loc[0.75]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "    df_out = df.copy()  # Create a copy to avoid modifying the original\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if outlier_percentages[col] <= threshold:\n",
    "            print(f\"Removing {outlier_percentages[col]:.2f}% outliers from column '{col}'.\")\n",
    "            df_out = df_out.assign(**{col: df[col].where((df[col] >= lower_bound[col]) & (df[col] <= upper_bound[col]))})\n",
    "        else:\n",
    "            print(f\"Applying Winsorization to column '{col}' with {outlier_percentages[col]:.2f}% outliers.\")\n",
    "            df_out = df_out.assign(**{col: df[col].map_partitions(lambda x: winsorize(x, limits=(0.05, 0.05)), meta=(col, \"float64\"))})\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "333116d2-fc7e-4c6d-b2ac-75a125afb300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:430: UserWarning: `kurtosistest` p-value may be inaccurate with fewer than 20 observations; only n=16 observations were given.\n",
      "  return hypotest_fun_in(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-selected method: iqr\n"
     ]
    }
   ],
   "source": [
    "outlier_percentages = detect_outliers_dask(df, method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a00a39-ebe8-4624-b2c1-bbf684f04240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 0.00% outliers from column 'Ladder'.\n",
      "Removing 0.00% outliers from column 'SD of Ladder'.\n",
      "Removing 0.00% outliers from column 'Positive affect'.\n",
      "Removing 0.00% outliers from column 'Negative affect'.\n",
      "Removing 0.00% outliers from column 'Social support'.\n",
      "Removing 0.00% outliers from column 'Freedom'.\n",
      "Removing 0.00% outliers from column 'Corruption'.\n",
      "Removing 0.00% outliers from column 'Generosity'.\n",
      "Removing 0.00% outliers from column 'Log of GDP\n",
      "per capita'.\n",
      "Removing 0.00% outliers from column 'Healthy life\n",
      "expectancy'.\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = clean_or_winsorize_dask(df, outlier_percentages, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ceede-b991-40a9-8c53-7f314ce36cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2fa9fe-930a-4021-aeac-bf755fdcda86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ✅ Convert FL_DATE to datetime format\n",
    "#df = df.map_partitions(lambda d: d.assign(FL_DATE=dd.to_datetime(d[\"FL_DATE\"], errors=\"coerce\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "960c1ffd-27ae-4100-af85-01af6ea9b357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Label Encoding to Country (region)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import LabelEncoder\n",
    "from dask import delayed\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\",\"string\"]).columns\n",
    "# ✅ Encode categorical variables\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique().compute()  # Get number of unique values\n",
    "    \n",
    "    if unique_count <= 10:\n",
    "        # **One-Hot Encoding** for low-cardinality categorical columns\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        \n",
    "        # Apply One-Hot Encoding using map_partitions\n",
    "        df = df.map_partitions(\n",
    "            lambda d: d.join(\n",
    "                pd.DataFrame(\n",
    "                    encoder.fit_transform(d[[col]]),\n",
    "                    index=d.index,\n",
    "                    columns=[f\"{col}_{i}\" for i in range(unique_count)]\n",
    "                )\n",
    "            ), \n",
    "            meta=df\n",
    "        )\n",
    "        \n",
    "        df = df.drop(columns=[col])  # Drop original column after encoding\n",
    "        print(f\"Applied One-Hot Encoding to {col}\")\n",
    "\n",
    "    else:\n",
    "        # **Label Encoding** for high-cardinality categorical columns\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = df[col].map_partitions(lambda x: encoder.fit_transform(x), meta=(col, 'int64'))\n",
    "        print(f\"Applied Label Encoding to {col}\")\n",
    "\n",
    "# ✅ Check the transformed DataFrame\n",
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96682549-10f9-46f9-b218-5323dc399b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Country (region)', 'Ladder', 'SD of Ladder', 'Positive affect',\n",
      "       'Negative affect', 'Social support', 'Freedom', 'Corruption',\n",
      "       'Generosity', 'Log of GDP\\nper capita', 'Healthy life\\nexpectancy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # Check if 'FL_DATE' exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f86934e1-0f4e-4e19-978b-15d2d79ec667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ✅ Identify column types\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'string[pyarrow]']).columns\n",
    "\n",
    "# ✅ Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# ✅ Function to scale only numerical columns, keeping categorical & datetime columns unchanged\n",
    "def scale_partition(partition):\n",
    "    partition[numerical_cols] = scaler.fit_transform(partition[numerical_cols].to_numpy())\n",
    "    return partition\n",
    "\n",
    "# ✅ Define `meta` using the correct column order from `df.columns`\n",
    "meta = {col: np.float64 for col in numerical_cols}  # Numerical columns\n",
    "meta.update({col: \"string\" for col in categorical_cols})  # Categorical columns\n",
    "meta[\"FL_DATE\"] = \"datetime64[ns]\"  # Ensure FL_DATE is properly formatted\n",
    "\n",
    "# ✅ Fix column order to match `df.columns`\n",
    "meta = {col: meta[col] for col in df.columns if col in meta}  # Reorder meta to match df.columns\n",
    "\n",
    "# ✅ Apply scaling in parallel using map_partitions\n",
    "df_scaled = df.map_partitions(scale_partition, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1329da4-c523-4eef-9041-e866d8d5b988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#✅ Preview the scaled data\n",
    "#df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f20bd7-6512-4491-943f-798c7e46272a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37e5b2eb-7db9-4d15-a6be-567e9b87fb63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "✅ Successfully uploaded processed data to: gs://demo-shubham/scaled-data.parquet\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from google.cloud import storage  # GCS upload\n",
    "\n",
    "# ✅ Define GCS output path\n",
    "output_path = \"gs://demo-shubham/scaled-data.parquet\"\n",
    "\n",
    "# ✅ Convert Dask DataFrame to Pandas in chunks & write to GCS\n",
    "gcs_client = storage.Client()\n",
    "bucket = gcs_client.bucket(\"demo-shubham\")\n",
    "blob = bucket.blob(\"scaled-data.parquet\")\n",
    "\n",
    "# ✅ Open a Parquet writer\n",
    "schema = pa.Schema.from_pandas(df_scaled.head())  # Infer schema from first few rows\n",
    "sink = pa.BufferOutputStream()  # Write in memory before uploading\n",
    "\n",
    "with pq.ParquetWriter(sink, schema=schema, compression=\"snappy\") as writer:\n",
    "    for i, partition in enumerate(df_scaled.to_delayed()):  # Iterate over Dask partitions\n",
    "        print(f\"Processing chunk {i + 1}...\")\n",
    "        df_chunk = partition.compute()  # Convert only this partition to Pandas\n",
    "        table = pa.Table.from_pandas(df_chunk)  # Convert to Apache Arrow table\n",
    "        writer.write_table(table)  # Append to Parquet file\n",
    "\n",
    "# ✅ Upload to GCS\n",
    "blob.upload_from_string(sink.getvalue().to_pybytes(), content_type=\"application/octet-stream\")\n",
    "print(f\"✅ Successfully uploaded processed data to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc7c91c-bd46-482e-886c-2ffba2bda220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Normalization.ipynb to script\n",
      "[NbConvertApp] Writing 10432 bytes to Normalization.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Normalization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3b1d1-c4ae-4a27-88fa-192cc385679f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84b9ab-c1de-4e7e-9484-43b0495a407c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
